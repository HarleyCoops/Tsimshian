import re
from collections import Counter

def tokenize_shmalgyack(text):
  """
  A basic hypothetical tokenizer for Shmalgyack.
  Splits by whitespace but handles intra-word apostrophes.
  Handles punctuation attached to words carefully.
  This would need refinement based on more data.
  Example: "Geegsh Dzon shu aad 'nee gya'wn." -> ['Geegsh', 'Dzon', 'shu', 'aad', "'nee", "gya'wn", '.']
  """
  # Simple whitespace split for now, better regex needed for real use
  # This regex attempts to keep words with internal apostrophes together
  # and separate punctuation. Needs testing and refinement.
  tokens = re.findall(r"[a-zA-Z'â€™]+(?:'[a-zA-Z]+)*|[.,!?;:]|[\w]+", text)
  return tokens

def get_normalized_frequency(item, tokens):
  """Calculates the frequency of an item per 100 tokens."""
  count = tokens.count(item)
  total_tokens = len(tokens)
  return (count * 100.0 / total_tokens) if total_tokens > 0 else 0.0

def get_char_frequency(char, text):
   """Calculates the frequency of a character per 100 characters."""
   count = text.count(char)
   total_chars = len(text)
   return (count * 100.0 / total_chars) if total_chars > 0 else 0.0

# Placeholder for identifying potential Proper Nouns (simple capitalization heuristic)
def get_potential_proper_nouns(tokens):
    return {token for token in tokens if token and token[0].isupper() and len(token) > 1}

# Placeholder for identifying potential Verbs (needs a lexicon or reference alignment)
def get_potential_verbs(tokens, reference_verbs):
    # In a real system, this requires alignment or a lexicon.
    # Simple approach: check overlap with known/reference verbs.
    return {token for token in tokens if token in reference_verbs}

# Placeholder for identifying potential Adjectives (needs lexicon/reference alignment)
def get_potential_adjectives(tokens, reference_adjectives):
    return {token for token in tokens if token in reference_adjectives}

# Placeholder for identifying potential Nouns (needs lexicon/reference alignment)
def get_potential_nouns(tokens, reference_nouns):
     return {token for token in tokens if token in reference_nouns}
 
 # Rule 1: Apostrophe Usage Frequency
def evaluate_apostrophe_frequency_rule1(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates adherence to the typical frequency of apostrophe usage.
    Compares the frequency of apostrophes (') per 100 characters in the candidate
    against the reference text. A score of 1.0 means identical frequency.
    Lower scores indicate deviation. Uses min/max ratio for grading.

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0.
    """
    ref_apostrophe_freq = get_char_frequency("'", reference_text)
    cand_apostrophe_freq = get_char_frequency("'", candidate_text)

    # Avoid division by zero if neither has apostrophes (perfect match in that case)
    if ref_apostrophe_freq == 0 and cand_apostrophe_freq == 0:
        return 1.0
    # If one has zero and the other doesn't, it's a maximal mismatch for frequency.
    elif ref_apostrophe_freq == 0 or cand_apostrophe_freq == 0:
        # Give a small score if the candidate correctly has zero,
        # but a penalty if it incorrectly has zero when ref has some.
        # A slightly harsher penalty if the candidate *adds* apostrophes where none exist.
        return 0.1 if cand_apostrophe_freq == 0 and ref_apostrophe_freq > 0 else 0.0
    else:
        # Graded score based on ratio similarity
        score = min(cand_apostrophe_freq, ref_apostrophe_freq) / max(cand_apostrophe_freq, ref_apostrophe_freq)
        return score

# Rule 2: Apostrophe Placement Correctness
def evaluate_apostrophe_placement_rule2(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates whether apostrophes in the candidate text precede likely consonants
    based on the rule ('d', 'k', 't', 'ts', 'w', 'y', 'l', 'm', 'n', 'b', 'g').
    Also checks if the words using apostrophes broadly match the reference.

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0 reflecting placement correctness.
    """
    allowed_following_chars = {'d', 'k', 't', 'w', 'y', 'l', 'm', 'n', 'b', 'g'} # 'ts' handled below
    tokens_cand = tokenize_shmalgyack(candidate_text)
    tokens_ref = tokenize_shmalgyack(reference_text)

    cand_apostrophe_tokens = {token for token in tokens_cand if "'" in token}
    ref_apostrophe_tokens = {token for token in tokens_ref if "'" in token}

    total_apostrophes_cand = candidate_text.count("'")
    correctly_placed = 0
    misplaced = 0

    if total_apostrophes_cand == 0:
        # If reference also has no apostrophes, perfect score. Otherwise, penalize.
        return 1.0 if reference_text.count("'") == 0 else 0.0

    for i, char in enumerate(candidate_text):
        if char == "'":
            # Check character immediately following
            if i + 1 < len(candidate_text):
                next_char = candidate_text[i+1].lower()
                # Handle 'ts' cluster
                if next_char == 't' and i + 2 < len(candidate_text) and candidate_text[i+2].lower() == 's':
                    correctly_placed += 1
                elif next_char in allowed_following_chars:
                     correctly_placed += 1
                # Allow apostrophe at start of token if followed by allowed char ('dsm)
                elif i == 0 or candidate_text[i-1].isspace():
                     if next_char in allowed_following_chars or (next_char == 't' and i + 2 < len(candidate_text) and candidate_text[i+2].lower() == 's'):
                         correctly_placed +=1
                     # Check specific examples like 'wee, 'yoota
                     elif candidate_text[i+1:i+4] == 'wee' or candidate_text[i+1:i+6] == 'yoota':
                          correctly_placed += 1
                     else:
                         misplaced += 1
                # Allow apostrophe after 'a' or 'u' before specific consonants like hla'ashg, lu'kwil
                elif i > 0 and candidate_text[i-1].lower() in 'au' and next_char in allowed_following_chars:
                     correctly_placed += 1
                else:
                     misplaced += 1
            else:
                # Apostrophe at the very end of the text is likely wrong
                misplaced += 1

    placement_score = correctly_placed / total_apostrophes_cand if total_apostrophes_cand > 0 else 1.0

    # Bonus/Penalty based on matching apostrophe-containing words with reference
    # Use Jaccard similarity for the sets of words containing apostrophes
    intersection = len(cand_apostrophe_tokens.intersection(ref_apostrophe_tokens))
    union = len(cand_apostrophe_tokens.union(ref_apostrophe_tokens))
    jaccard_sim = intersection / union if union > 0 else 1.0 # Perfect if no apostrophes in either

    # Combine scores: placement correctness is primary, Jaccard match provides refinement
    final_score = (0.7 * placement_score) + (0.3 * jaccard_sim)

    return final_score


# Rule 3: Double Consonant 'gg' Frequency
def evaluate_double_gg_frequency_rule3(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates adherence to the typical frequency of the double consonant 'gg'.
    Compares frequency per 100 tokens in candidate vs. reference.

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0 based on frequency similarity.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text.lower()) # Lowercase for freq matching
    ref_tokens = tokenize_shmalgyack(reference_text.lower())

    cand_gg_freq = sum(1 for token in cand_tokens if 'gg' in token) * 100.0 / len(cand_tokens) if cand_tokens else 0.0
    ref_gg_freq = sum(1 for token in ref_tokens if 'gg' in token) * 100.0 / len(ref_tokens) if ref_tokens else 0.0

    if ref_gg_freq == 0 and cand_gg_freq == 0:
        return 1.0
    elif ref_gg_freq == 0 or cand_gg_freq == 0:
        # Penalize adding 'gg' where none exists, slightly less penalty for omitting it
        return 0.1 if cand_gg_freq == 0 else 0.0
    else:
        return min(cand_gg_freq, ref_gg_freq) / max(cand_gg_freq, ref_gg_freq)


# Rule 4: Double Vowel 'aa', 'ee' Frequency
def evaluate_double_vowel_frequency_rule4(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates adherence to typical frequency of double vowels 'aa', 'ee'.
    Compares combined frequency per 100 tokens in candidate vs. reference.

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0 based on frequency similarity.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text.lower())
    ref_tokens = tokenize_shmalgyack(reference_text.lower())

    cand_dv_freq = sum(1 for token in cand_tokens if 'aa' in token or 'ee' in token) * 100.0 / len(cand_tokens) if cand_tokens else 0.0
    ref_dv_freq = sum(1 for token in ref_tokens if 'aa' in token or 'ee' in token) * 100.0 / len(ref_tokens) if ref_tokens else 0.0

    if ref_dv_freq == 0 and cand_dv_freq == 0:
        return 1.0
    elif ref_dv_freq == 0 or cand_dv_freq == 0:
        return 0.1 if cand_dv_freq == 0 else 0.0
    else:
        return min(cand_dv_freq, ref_dv_freq) / max(cand_dv_freq, ref_dv_freq)

# Rule 5: Common Consonant Cluster Usage
def evaluate_common_clusters_rule5(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates the usage of common Shmalgyack consonant clusters.
    Checks presence and relative frequency of 'hl', 'ck', 'ksh', 'shg', 'ds', 'ts', 'tk', 'gw', 'kw'.

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0 reflecting appropriate cluster usage.
    """
    clusters = ['hl', 'ck', 'ksh', 'shg', 'ds', 'ts', 'tk', 'gw', 'kw']
    cand_text_lower = candidate_text.lower()
    ref_text_lower = reference_text.lower()

    cand_cluster_counts = {cl: cand_text_lower.count(cl) for cl in clusters}
    ref_cluster_counts = {cl: ref_text_lower.count(cl) for cl in clusters}

    total_cand_clusters = sum(cand_cluster_counts.values())
    total_ref_clusters = sum(ref_cluster_counts.values())

    if total_ref_clusters == 0 and total_cand_clusters == 0:
        return 1.0
    if total_ref_clusters == 0 or total_cand_clusters == 0:
        # Penalize mismatch significantly
        return 0.05 if total_cand_clusters == 0 else 0.0 # Small reward for correctly having none

    score_sum = 0.0
    num_clusters_present_in_ref = 0

    for cl in clusters:
        ref_count = ref_cluster_counts[cl]
        cand_count = cand_cluster_counts[cl]

        # Consider only clusters present in the reference for calculating the score
        if ref_count > 0:
            num_clusters_present_in_ref += 1
            if cand_count == 0:
                score_sum += 0.0 # Penalize omission
            else:
                # Relative frequency score (normalized by total clusters)
                cand_rel_freq = cand_count / total_cand_clusters if total_cand_clusters > 0 else 0
                ref_rel_freq = ref_count / total_ref_clusters if total_ref_clusters > 0 else 0
                score_sum += min(cand_rel_freq, ref_rel_freq) / max(cand_rel_freq, ref_rel_freq) if max(cand_rel_freq, ref_rel_freq) > 0 else 1.0

    # Normalize the score based on how many clusters were expected (present in ref)
    average_score = score_sum / num_clusters_present_in_ref if num_clusters_present_in_ref > 0 else 1.0

    # Penalty for introducing clusters not found in the reference text segment
    penalty = 0
    num_introduced = 0
    for cl in clusters:
       if cand_cluster_counts[cl] > 0 and ref_cluster_counts[cl] == 0:
           num_introduced +=1
    # Apply penalty based on proportion of 'new' clusters introduced
    penalty_factor = 1.0 - (num_introduced / len(clusters)) * 0.5 # Max penalty 0.5
    # The penalty shouldn't make the score negative or zero if it was otherwise good. Reduce proportionally.
    final_score = average_score * penalty_factor

    return final_score


# Rule 6: Absence of Certain English Letters (Q, X, V)
def evaluate_forbidden_letters_rule6(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates the absence of forbidden letters (Q, X, V) in common words.
    Allows them only if they appear in capitalized words (assumed Proper Nouns)
    that are also present in the reference text (e.g., handling loanwords/names).

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0. 1.0 means no forbidden letters inappropriately used.
    """
    forbidden_letters = {'q', 'x', 'v'}
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)
    ref_proper_nouns = get_potential_proper_nouns(ref_tokens) # Identify potential proper nouns in ref

    violations = 0
    total_chars = len(candidate_text)
    if total_chars == 0:
        return 1.0

    num_checked_tokens = 0
    for token in cand_tokens:
        is_potential_proper = token and token[0].isupper() and len(token) > 1
        # Allow forbidden letter only if it's in a capitalized token that ALSO exists in the reference proper nouns list.
        # This handles cases like "Everett", "Viet Nam".
        allowed_context = is_potential_proper and token in ref_proper_nouns

        if not allowed_context:
           num_checked_tokens +=1
           for char in token.lower():
               if char in forbidden_letters:
                   violations += 1

    # Score inversely proportional to violation density
    violation_density = violations / num_checked_tokens if num_checked_tokens > 0 else 0
    score = max(0.0, 1.0 - violation_density * 5.0) # Amplify penalty: even few violations are bad

    return score

# Rule 7: Valid Word Endings
def evaluate_word_endings_rule7(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates whether words in the candidate text end with common Shmalgyack endings.
    Checks against vowels (a, u, i, o, e) or consonants (k, t, d, sh, m, n, g, l).

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text (used for context, less direct compare).

    Returns:
        A score between 0.0 and 1.0. 1.0 means all words have valid endings.
    """
    # Define valid endings (single chars or 'sh')
    valid_single_char_endings = {'a', 'u', 'i', 'o', 'e', 'k', 't', 'd', 'm', 'n', 'g', 'l'}

    cand_tokens = tokenize_shmalgyack(candidate_text.lower())
    # Filter out punctuation tokens if tokenizer separates them
    cand_tokens = [token for token in cand_tokens if token and token.isalnum()]

    if not cand_tokens:
        return 1.0 # Empty candidate has no violations

    valid_ending_count = 0
    for token in cand_tokens:
        if not token: continue
        # Check for 'sh' ending
        if token.endswith('sh'):
            valid_ending_count += 1
        # Check for single character endings
        elif token[-1] in valid_single_char_endings:
            valid_ending_count += 1

    score = valid_ending_count / len(cand_tokens)
    return score

# Rule 8: Use of 'h' (Frequency and Placement - Basic)
def evaluate_h_usage_rule8(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates the usage of the letter 'h', focusing on overall frequency
    and basic positional occurrence (initial, medial) compared to the reference.

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text.lower())
    ref_tokens = tokenize_shmalgyack(reference_text.lower())

    # 1. Overall Frequency Score
    cand_h_freq = get_char_frequency('h', candidate_text.lower())
    ref_h_freq = get_char_frequency('h', reference_text.lower())

    if ref_h_freq == 0 and cand_h_freq == 0:
        freq_score = 1.0
    elif ref_h_freq == 0 or cand_h_freq == 0:
        freq_score = 0.1 if cand_h_freq == 0 else 0.0
    else:
        freq_score = min(cand_h_freq, ref_h_freq) / max(cand_h_freq, ref_h_freq)

    # 2. Positional Score (Initial 'h' Rate) - Simplified check
    cand_initial_h = sum(1 for token in cand_tokens if token.startswith('h'))
    ref_initial_h = sum(1 for token in ref_tokens if token.startswith('h'))

    cand_initial_rate = cand_initial_h / len(cand_tokens) if cand_tokens else 0
    ref_initial_rate = ref_initial_h / len(ref_tokens) if ref_tokens else 0

    if ref_initial_rate == 0 and cand_initial_rate == 0:
        pos_score = 1.0
    elif ref_initial_rate == 0 or cand_initial_rate == 0:
        pos_score = 0.1 if cand_initial_rate == 0 else 0.0
    else:
        pos_score = min(cand_initial_rate, ref_initial_rate) / max(cand_initial_rate, ref_initial_rate)

    # Combine scores (e.g., 60% frequency, 40% initial position)
    final_score = (0.6 * freq_score) + (0.4 * pos_score)
    return final_score


# Rule 9: Case Sensitivity (Proper Nouns vs Common Nouns)
def evaluate_case_sensitivity_rule9(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates correct capitalization for potential proper nouns (names, places)
    and lowercasing for common nouns, by comparing against the reference casing.

    Args:
        candidate_text: The text generated by the language model.
        reference_text: The ground truth Shmalgyack text.

    Returns:
        A score between 0.0 and 1.0. 1.0 indicates perfect case matching.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)

    # Need alignment or at least matching token sequences for robust check.
    # Simple approximation: check tokens present in both.
    cand_vocab = {token: token[0].isupper() for token in cand_tokens if token.isalnum()}
    ref_vocab = {token: token[0].isupper() for token in ref_tokens if token.isalnum()}
    # Identify potential proper nouns based on capitalization IN THE REFERENCE
    ref_proper_nouns_case = {token for token, is_upper in ref_vocab.items() if is_upper}
    ref_common_nouns_case = {token for token, is_upper in ref_vocab.items() if not is_upper}

    correct_case_count = 0
    total_comparable_tokens = 0

    # Check common tokens and candidate-specific tokens
    processed_tokens = set() # Avoid double counting if token appears multiple times

    for token in cand_tokens:
        if not token.isalnum() or token in processed_tokens:
            continue
        processed_tokens.add(token)
        total_comparable_tokens += 1
        cand_is_upper = cand_vocab.get(token, token[0].isupper()) # Case in candidate

        # Case 1: Token also exists in reference - must match reference casing
        if token in ref_vocab:
             ref_is_upper = ref_vocab[token]
             if cand_is_upper == ref_is_upper:
                 correct_case_count += 1
        # Case 2: Token *only* in candidate - infer desired casing
        # Heuristic: If it resembles (lowercase version) a proper noun from ref -> should be Proper? Risky.
        # Simpler: Assume new common nouns should be lowercase, unless they start a sentence.
        # We can't easily detect sentence starts reliably without more parsing.
        # Let's penalize *capitalized* novel words more heavily, unless they look like known ref proper nouns.
        elif token.lower() in [t.lower() for t in ref_proper_nouns_case]:
             # If it looks like a known Proper Noun, demand capitalization
             if cand_is_upper:
                  correct_case_count += 0.8 # Give partial credit for plausible casing
             # else: penalty (implicit as correct_case_count doesn't increase)
        elif cand_is_upper and not (token.lower() in [t.lower() for t in ref_proper_nouns_case]):
             # Novel capitalized word not resembling known proper noun - likely error
             correct_case_count += 0.2 # Small score - maybe it IS a new proper noun?
        else: # Novel lowercase word - likely correct for a common noun
             correct_case_count += 1.0


    score = correct_case_count / total_comparable_tokens if total_comparable_tokens > 0 else 1.0
    return score

# Rule 10: Lack of Silent Letters (Inferred) - HARD TO EVALUATE directly
# This is more of a principle guiding other rules. We won't create a specific function
# for this inference, as it's implicitly checked by matching known words and phonotactic
# rules (like allowed clusters, endings). We could potentially penalize unattested
# character sequences, but that overlaps with other rules.

# ---

**II. Word Order Evaluation Functions**

```python
# Requires more advanced parsing or strong alignment with reference.
# We'll use simple heuristics and reference comparison. Assume sentence-level texts.

# Rule 11: Verb Position (VSO/VOS Tendency)
def evaluate_verb_position_rule11(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if verbs appear early (pos 0, 1, 2) in the candidate clause,
    approximating a VSO/VOS tendency by checking against reference verb position.
    NEEDS reliable verb identification.

    Args:
        candidate_text: The generated sentence/clause.
        reference_text: The ground truth sentence/clause.

    Returns:
        Score (0.0-1.0) indicating adherence to early verb placement.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)

    # *** Placeholder: Identify Verbs - Needs a Lexicon or Alignment ***
    # Let's assume we can identify the main verb(s) in both candidate and reference
    # E.g., by finding tokens matching a known verb list derived from reference/training data.
    ref_verbs = {"Geegsh", "Needsu", "yaawckga'nu", "baasha'nu", "kshwa'dackga", "hultga", "dalbikshga", "algyackd", "meelgd"} # Example subset
    cand_verb_indices = [i for i, token in enumerate(cand_tokens) if token in ref_verbs]
    ref_verb_indices = [i for i, token in enumerate(ref_tokens) if token in ref_verbs]

    if not ref_verb_indices:
        # If reference has no identifiable verb in this segment, evaluate consistency
        return 1.0 if not cand_verb_indices else 0.0 # Penalize adding verb if none in ref

    if not cand_verb_indices:
        return 0.0 # Penalize omitting verb present in ref

    # Compare the position of the *first* identified verb
    cand_first_verb_pos = cand_verb_indices[0]
    ref_first_verb_pos = ref_verb_indices[0]

    # Ideal score if candidate verb position matches reference position *and* is early
    is_early = lambda pos, length: pos <= 2 or pos <= length / 3 # Heuristic for 'early'

    cand_early = is_early(cand_first_verb_pos, len(cand_tokens))
    ref_early = is_early(ref_first_verb_pos, len(ref_tokens))

    if cand_early and ref_early:
         # Bonus if absolute positions also match
        score = 1.0 if cand_first_verb_pos == ref_first_verb_pos else 0.9
    elif cand_early and not ref_early:
         score = 0.3 # Model is making it VSO/VOS but ref wasn't? Low score.
    elif not cand_early and ref_early:
         score = 0.2 # Model failed to make it VSO/VOS when ref was. High penalty.
    else: # Neither is early (according to heuristic)
        # If positions match despite being late, give some credit
        score = 0.6 if cand_first_verb_pos == ref_first_verb_pos else 0.4

    return score


# Rule 13: Adjective Placement (Preceding Noun)
def evaluate_adjective_placement_rule13(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if adjectives precede the nouns they modify.
    NEEDS reliable Adjective/Noun identification & relationship mapping.
    Approximation: Check sequences found in reference (e.g., "shu aad").

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.

    Returns:
        Score (0.0-1.0) based on correct Adj-Noun ordering.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)

    # *** Placeholder: Identify Adj/Nouns & Relationships ***
    # Example known pairs from rules: ('shu', 'aad'), ("'wee", 'ggan'), ('hlgu', 'hash')
    ref_adj_noun_pairs = {('shu', 'aad'), ("'wee", 'ggan'), ('hlgu', 'hash')} # Needs expanding

    correct_placements = 0
    total_opportunities = 0

    # Check candidate for known Adj-Noun sequences from reference examples
    for i in range(len(cand_tokens) - 1):
        pair = (cand_tokens[i], cand_tokens[i+1])
        if pair in ref_adj_noun_pairs:
            total_opportunities += 1
            correct_placements += 1 # Assumes format Adj Noun is correct

    # Look for reversed pairs (potential errors)
    for i in range(len(cand_tokens) - 1):
         pair = (cand_tokens[i+1], cand_tokens[i]) # Check Noun Adj order
         if pair in ref_adj_noun_pairs:
              total_opportunities += 1 # Counts as an opportunity where error occurred
              # No increment to correct_placements

    # How many relevant pairs from reference *should* have appeared?
    expected_pairs_count = 0
    for adj, noun in ref_adj_noun_pairs:
         # Crude check: if both words appear in reference, assume pair was intended
         if adj in ref_tokens and noun in ref_tokens:
              # A better check would see if they are close in the ref text
              expected_pairs_count += 1

    # Calculate score based on correct placements found vs opportunities
    # Give baseline score related to how many expected pairs were even attempted
    coverage_score = total_opportunities / expected_pairs_count if expected_pairs_count > 0 else 1.0
    placement_score = correct_placements / total_opportunities if total_opportunities > 0 else 1.0

    # Combine: good placement within covered pairs, weighted by coverage
    # If coverage is low, the placement score is less meaningful.
    final_score = placement_score * (0.5 + 0.5 * coverage_score)

    return final_score

# Rule 14: Possessive Pronoun Placement (Preceding Noun)
def evaluate_possessive_pronoun_placement_rule14(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if possessive pronouns ('yagwa', 'awaa') precede nouns.

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.

    Returns:
        Score (0.0-1.0) based on correct PossPron-Noun ordering.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    possessives = {'yagwa', 'awaa'}

    correct_placements = 0
    instances_found = 0

    for i, token in enumerate(cand_tokens):
        if token in possessives:
            instances_found += 1
            # Check if next token exists and looks like a noun (heuristic: not another possessive/verb/func word)
            if i + 1 < len(cand_tokens):
                 # A basic check: is the next word NOT another particle/possessive? Crude.
                 # Better: Use reference alignment or noun list.
                 next_token = cand_tokens[i+1]
                 # Let's assume most words following possessives *should* be nouns for now
                 if next_token not in possessives and next_token.isalnum(): # Simple heuristic
                       correct_placements += 1

    if instances_found == 0:
        # Check if reference *expected* possessives here. If not, score 1.
        ref_has_poss = any(p in reference_text for p in possessives)
        return 1.0 if not ref_has_poss else 0.0 # Penalize if expected but missing

    score = correct_placements / instances_found
    return score

# Rule 15: Negator 'Ahlgadee' Placement (Before Negated Element)
def evaluate_negator_placement_rule15(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if the negator 'Ahlgadee' (and variants) precedes the element it negates,
    often comparing its relative position to the reference structure.

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.

    Returns:
        Score (0.0-1.0) indicating correct negator placement.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)
    negators = {'Ahlgadee', 'ahlga', 'ahlgandee'} # Include variants

    cand_neg_indices = [i for i, token in enumerate(cand_tokens) if token in negators]
    ref_neg_indices = [i for i, token in enumerate(ref_tokens) if token in negators]

    if not ref_neg_indices:
        return 1.0 if not cand_neg_indices else 0.0 # Penalize adding spurious negators

    if not cand_neg_indices:
        return 0.0 # Penalize omitting required negators

    # Compare position relative to the *next* word (often the negated verb/adj)
    # Simple comparison assuming single negation per segment for simplicity
    cand_neg_pos = cand_neg_indices[0]
    ref_neg_pos = ref_neg_indices[0]

    # Check if negator is immediately followed by something (not sentence end)
    cand_follows = cand_neg_pos + 1 < len(cand_tokens)
    ref_follows = ref_neg_pos + 1 < len(ref_tokens)

    if not cand_follows:
        return 0.1 # Negator shouldn't be dangling

    # Score higher if negator is placed correctly *before* the presumably negated element.
    # The most robust check compares alignment to reference.
    # Heuristic: Does the word *after* the negator in candidate match the word *after* negator in reference?
    cand_next_word = cand_tokens[cand_neg_pos + 1] if cand_follows else None
    ref_next_word = ref_tokens[ref_neg_pos + 1] if ref_follows else None

    # Score based on position matching ref position, and if context matches
    pos_match_score = 1.0 if cand_neg_pos == ref_neg_pos else 0.6
    context_match_score = 1.0 if cand_next_word == ref_next_word else 0.5

    # Combine scores, penalizing heavily if context is wrong
    score = pos_match_score * context_match_score
    return score


# Rule 16: Preposition ('da', 'lack') Placement (Before Noun Phrase)
def evaluate_preposition_placement_rule16(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if prepositions 'da', 'lack' precede a likely noun/noun phrase.

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text (context).

    Returns:
        Score (0.0-1.0) based on correct Prep-NP ordering.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    prepositions = {'da', 'lack'}

    correct_placements = 0
    instances_found = 0

    for i, token in enumerate(cand_tokens):
        if token in prepositions:
            instances_found += 1
            # Check if next token exists and is likely a noun/determiner (not another prep/verb etc.)
            if i + 1 < len(cand_tokens):
                 next_token = cand_tokens[i+1]
                 # Heuristic: Next token is alphanumeric and not a known preposition/verb? Needs noun list ideally.
                 if next_token.isalnum() and next_token not in prepositions: # Add known verbs/particles to exclude if needed
                     correct_placements += 1

    if instances_found == 0:
        ref_has_prep = any(p in reference_text for p in prepositions)
        return 1.0 if not ref_has_prep else 0.0

    score = correct_placements / instances_found
    return score


# Rule 18: Question Word Order (Starts with Interrogative)
def evaluate_question_word_order_rule18(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if candidate questions start with known interrogative words.
    Assumes input texts are single questions.

    Args:
        candidate_text: The generated question.
        reference_text: The ground truth question.

    Returns:
        Score (0.0-1.0). 1.0 if starts correctly, 0.0 otherwise.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)
    interrogatives = {'Ndahl', 'Naahl', 'Goahl'} # Add 'Wil' when used interrogatively based on context if possible

    is_ref_question = reference_text.endswith('?') or (ref_tokens and ref_tokens[0] in interrogatives)
    is_cand_question = candidate_text.endswith('?') # Crude check

    if not is_ref_question:
         # If reference isn't a question, candidate shouldn't be framed as one
        return 1.0 if not (cand_tokens and cand_tokens[0] in interrogatives) else 0.1

    # Reference is a question, candidate should be too
    if not cand_tokens: return 0.0 # Empty candidate can't be a question

    cand_starts_correctly = cand_tokens[0] in interrogatives

    if cand_starts_correctly:
        # Bonus if the interrogative word matches the reference interrogative
        ref_interrogative = ref_tokens[0] if ref_tokens and ref_tokens[0] in interrogatives else None
        return 1.0 if cand_tokens[0] == ref_interrogative else 0.8
    else:
        return 0.0 # Failed to start with a question word


# Rule 19: 'Gwee' Placement (Often After Noun)
def evaluate_gwee_placement_rule19(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if the demonstrative(?) 'gwee' appears *after* a likely noun,
    reflecting the observed pattern (e.g., "'yoota gwee").

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text (context, alignment).

    Returns:
        Score (0.0-1.0) based on 'gwee' following a noun-like word.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text) # Used for checking expectation

    correct_placements = 0
    instances_found = 0

    for i, token in enumerate(cand_tokens):
        if token == 'gwee':
            instances_found += 1
            # Check if the *previous* token exists and is noun-like
            if i > 0:
                prev_token = cand_tokens[i-1]
                # Heuristic: Previous token is alphanumeric and not a known function word/particle? Needs Noun list.
                if prev_token.isalnum() and prev_token not in {'da', 'lack', 'hla', 'wil', 'ada', 'nee', 'dm', 'gwee'}: # Basic check
                    correct_placements += 1

    if instances_found == 0:
        ref_has_gwee = 'gwee' in ref_tokens
        return 1.0 if not ref_has_gwee else 0.0

    score = correct_placements / instances_found
    return score

# ---

**III & IV. Morphology (Prefixes & Suffixes) Evaluation Functions**

These require comparing word forms. A simple approach checks if words known to have affixes *in the reference* also have them (or plausible variations) in the candidate at corresponding positions (requires alignment or bag-of-words comparison).

```python
# Rule 20: Diminutive Prefix 'hlgu-' Usage
def evaluate_prefix_hlgu_rule20(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates correct usage of the 'hlgu-' diminutive prefix.
    Checks if words prefixed with 'hlgu-' in the candidate correspond
    to semantically appropriate words (ideally 'hlgu-' prefixed) in the reference.

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.

    Returns:
        Score (0.0-1.0) based on Precision and Recall of 'hlgu-' usage.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text.lower())
    ref_tokens = tokenize_shmalgyack(reference_text.lower())

    cand_hlgu_words = {token for token in cand_tokens if token.startswith('hlgu-')}
    ref_hlgu_words = {token for token in ref_tokens if token.startswith('hlgu-')}

    # Simple Set comparison (Precision/Recall/F1)
    true_positives = len(cand_hlgu_words.intersection(ref_hlgu_words))
    false_positives = len(cand_hlgu_words.difference(ref_hlgu_words))
    false_negatives = len(ref_hlgu_words.difference(cand_hlgu_words))

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 1.0 if not cand_hlgu_words else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 1.0 if not ref_hlgu_words else 0.0

    # F1 Score for balanced measure
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 1.0 if not cand_hlgu_words and not ref_hlgu_words else 0.0

    return f1_score


# Generic Affix Evaluation Function (Adaptable for Rules 21-36)
def evaluate_affix_usage(candidate_text: str, reference_text: str, affix: str, type: str = 'prefix', position: str = 'start') -> float:
    """
    Generic function to evaluate the usage of a specific prefix or suffix.
    Calculates F1 score based on matching words with the affix between candidate and reference.

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.
        affix: The prefix/suffix string to check (e.g., 'aam-', '-nu').
        type: 'prefix' or 'suffix'.
        position: 'start' or 'end' (used by startswith/endswith).

    Returns:
        F1 score (0.0-1.0) for the affix usage.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text.lower())
    ref_tokens = tokenize_shmalgyack(reference_text.lower())

    if position == 'start':
        cand_affixed_words = {token for token in cand_tokens if token.startswith(affix)}
        ref_affixed_words = {token for token in ref_tokens if token.startswith(affix)}
    elif position == 'end':
        cand_affixed_words = {token for token in cand_tokens if token.endswith(affix)}
        ref_affixed_words = {token for token in ref_tokens if token.endswith(affix)}
    else: # Could add 'contains' or more complex logic later
        raise ValueError("Position must be 'start' or 'end'")

    true_positives = len(cand_affixed_words.intersection(ref_affixed_words))
    false_positives = len(cand_affixed_words.difference(ref_affixed_words))
    false_negatives = len(ref_affixed_words.difference(cand_affixed_words))

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 1.0 if not cand_affixed_words else 0.0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 1.0 if not ref_affixed_words else 0.0

    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 1.0 if not cand_affixed_words and not ref_affixed_words else 0.0

    return f1_score

# Example Application for other affixes:
# evaluate_prefix_aam_rule21 = lambda cand, ref: evaluate_affix_usage(cand, ref, 'aam-', 'prefix', 'start')
# evaluate_suffix_nu_rule27 = lambda cand, ref: evaluate_affix_usage(cand, ref, '-nu', 'suffix', 'end')
# evaluate_suffix_ga_rule28 = lambda cand, ref: evaluate_affix_usage(cand, ref, '-ga', 'suffix', 'end')
# ...and so on for Rules 21-36. Need careful definition of affix string ('-t' vs part of root? '-ck'?)
# Some rules need more nuance than simple startswith/endswith (e.g. -t/-d/-k could be alternatives).
# A more advanced version could check stems or use morphological analysis if available.

# Rule 37: Future Marker 'Dm' Usage
def evaluate_future_marker_dm_rule37(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates the correct usage of the future/intent marker 'Dm'.
    Checks presence/absence consistency and position (typically near verb) against reference.

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.

    Returns:
        F1 score (0.0-1.0) reflecting correct 'Dm' usage.
    """
    # Use the generic affix function concept, treating 'Dm' as a distinct word/token.
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)

    # Check presence/absence consistency (like F1 score)
    cand_has_dm = 'Dm' in cand_tokens
    ref_has_dm = 'Dm' in ref_tokens

    if cand_has_dm == ref_has_dm:
         presence_score = 1.0
         if not cand_has_dm: # Neither has it, perfect score
             return 1.0
    else:
         presence_score = 0.0 # Mismatch in presence

    # Check positional consistency (simple check: is it near the same word as in reference?)
    # Find index of 'Dm' and compare neighbors
    try:
         cand_dm_idx = cand_tokens.index('Dm')
         ref_dm_idx = ref_tokens.index('Dm')

         # Check word immediately after 'Dm' if possible
         cand_next = cand_tokens[cand_dm_idx + 1] if cand_dm_idx + 1 < len(cand_tokens) else None
         ref_next = ref_tokens[ref_dm_idx + 1] if ref_dm_idx + 1 < len(ref_tokens) else None

         pos_score = 0.5
         if cand_next and ref_next and cand_next == ref_next:
              pos_score = 1.0 # High confidence if context word matches
         elif abs(cand_dm_idx - ref_dm_idx) <= 1 : # Allow minor shift if context differs
             pos_score = 0.8

    except ValueError:
         # Should not happen if presence_score isn't 0, but handle defensively
         pos_score = 0.0 if presence_score > 0 else 1.0 # If Dm shouldn't be there, position is irrelevant.


    # Combine: Must be present correctly (presence_score), and position should be reasonable (pos_score)
    final_score = presence_score * pos_score
    return final_score

# Generic Function Word Evaluation (Adaptable for 38-47, 52, 53)
def evaluate_function_word_usage(candidate_text: str, reference_text: str, func_word: str, check_pos: bool = True) -> float:
    """
    Generic function to evaluate the usage of key function words/particles.
    Checks for presence/absence consistency (F1-like) and optionally positional similarity.

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.
        func_word: The function word/particle to evaluate (case-sensitive).
        check_pos: Whether to perform a simple positional check.

    Returns:
        Score (0.0-1.0) for function word usage.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)

    cand_indices = [i for i, token in enumerate(cand_tokens) if token == func_word]
    ref_indices = [i for i, token in enumerate(ref_tokens) if token == func_word]

    # Presence/Absence F1 calculation (approximated)
    tp = sum(1 for idx in cand_indices if idx in ref_indices) # Crude alignment check: matching index
    tp = min(len(cand_indices), len(ref_indices)) # Simpler: count min overlap assuming bag of words ok for presence

    fp = max(0, len(cand_indices) - len(ref_indices))
    fn = max(0, len(ref_indices) - len(cand_indices))

    precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0 if not cand_indices else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 1.0 if not ref_indices else 0.0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 1.0 if not cand_indices and not ref_indices else 0.0
    presence_score = f1

    if not check_pos or not cand_indices or not ref_indices:
         return presence_score # Return presence score if position check not needed or impossible


    # Positional Score (Average similarity of indices for multiple occurrences?)
    # Simplification: compare position of first occurrence if exists
    cand_first_pos = cand_indices[0]
    ref_first_pos = ref_indices[0]

    # Consider position relative to sequence length
    cand_rel_pos = cand_first_pos / len(cand_tokens) if cand_tokens else 0
    ref_rel_pos = ref_first_pos / len(ref_tokens) if ref_tokens else 0

    # Score based on proximity of relative positions
    pos_similarity = max(0.0, 1.0 - abs(cand_rel_pos - ref_rel_pos) * 2.0) # Penalize large shifts

    # Combine presence F1 and positional similarity
    final_score = (0.6 * presence_score) + (0.4 * pos_similarity)

    return final_score


# Apply for rules:
# evaluate_negator_ahlgadee_rule38 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Ahlgadee', True) # And variants? Requires more logic.
# evaluate_determiner_hla_rule39 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Hla', True)
# evaluate_preposition_da_rule40 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Da', True)
# evaluate_word_wil_rule41 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Wil', True)
# evaluate_noun_gyad_rule42 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'gyad', False) # Position maybe less critical?
# evaluate_demonstrative_gwee_rule43 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'gwee', True) # Covered better by Rule 19
# evaluate_determiner_lack_rule44 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Lack', True) # Covered better by Rule 16
# evaluate_verb_nee_rule45 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Nee', True)
# evaluate_conjunction_ada_rule46 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Ada', True)
# evaluate_command_giloa_rule52 = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'Giloa', True)
# evaluate_particle_sha_rule53a = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'sha', True)
# evaluate_particle_ga_rule53b = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'ga', True)
# evaluate_particle_ap_rule53c = lambda cand, ref: evaluate_function_word_usage(cand, ref, 'ap', True)


# Rule 48: Absence of Copula Check
def evaluate_copula_absence_rule48(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if the candidate correctly omits an explicit copula verb (like English 'is/are')
    in stative sentences where the reference also omits it (or uses 'hla' functionally).

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.

    Returns:
        Score (0.0-1.0). 1.0 if candidate structure matches reference regarding copula.
    """
    # This is hard without POS tagging or semantic understanding.
    # Heuristic: If reference contains likely adjective + noun without verb, does candidate too?
    # Example: "Aadsacka hagwilhoo" -> Adj + Noun (no verb).
    cand_tokens = tokenize_shmalgyack(candidate_text)
    ref_tokens = tokenize_shmalgyack(reference_text)

    # *** Needs identification of Stative Verbs/Adjectives & Lack of Verb ***
    # Let's assume a very simplified check based on 'hla' usage in potential copula contexts.
    cand_has_hla = 'hla' in [t.lower() for t in cand_tokens]
    ref_has_hla = 'hla' in [t.lower() for t in ref_tokens]

    # Basic check: Does 'hla' usage match? (assuming 'hla' might function as copula)
    # A more sophisticated check would identify contexts like [Noun/Adj] [Noun/Adj] expected in ref.
    if cand_has_hla == ref_has_hla:
         # Further check: If reference lacks common verbs AND hla, does candidate also lack them?
         ref_lacks_common_verb = not any(v in ref_tokens for v in {"Geegsh", "Needsu", "algyackd"}) # Example verbs
         cand_lacks_common_verb = not any(v in cand_tokens for v in {"Geegsh", "Needsu", "algyackd"})

         if ref_lacks_common_verb and not ref_has_hla:
              return 1.0 if cand_lacks_common_verb and not cand_has_hla else 0.2 # Penalize adding verb/hla unnecessarily
         else:
             return 1.0 # Assume match if hla usage matches or case is not the simple copula-drop one
    else:
         return 0.3 # Mismatch in potential 'hla' copula usage.

    # This function is highly speculative due to lack of linguistic analysis tools.

# Rule 50: Compounding Structure
def evaluate_compounding_rule50(candidate_text: str, reference_text: str) -> float:
    """
    Evaluates if candidate uses juxtaposition similar to reference for concepts
    potentially expressed as compounds (e.g., "shameeym wun" for 'deer meat').

    Args:
        candidate_text: The generated text.
        reference_text: The ground truth text.

    Returns:
        Score (0.0-1.0) reflecting similarity in handling compound-like structures.
    """
    cand_tokens = tokenize_shmalgyack(candidate_text.lower())
    ref_tokens = tokenize_shmalgyack(reference_text.lower())

    # Potential compounds from reference examples (Noun + Noun)
    ref_compounds_pairs = {('shameeym', 'wun'), ('waabm', 'hash')} # Needs expansion

    found_in_cand = 0
    expected_in_ref = 0
    structurally_matched = 0

    # Check how reference expresses these concepts
    for n1, n2 in ref_compounds_pairs:
        if n1 in ref_tokens and n2 in ref_tokens:
            expected_in_ref += 1
            try:
                 idx1 = ref_tokens.index(n1)
                 idx2 = ref_tokens.index(n2)
                 # Check if they appear close together in reference (juxtaposed)
                 ref_is_juxtaposed = abs(idx1 - idx2) == 1

                 # Now check candidate
                 if n1 in cand_tokens and n2 in cand_tokens:
                     found_in_cand +=1
                     cand_idx1 = cand_tokens.index(n1)
                     cand_idx2 = cand_tokens.index(n2)
                     cand_is_juxtaposed = abs(cand_idx1 - cand_idx2) == 1

                     if ref_is_juxtaposed == cand_is_juxtaposed:
                         structurally_matched += 1
                     elif cand_is_juxtaposed and not ref_is_juxtaposed:
                          # Candidate created juxtaposition where ref didn't? Minor penalty.
                          structurally_matched += 0.5
                     elif not cand_is_juxtaposed and ref_is_juxtaposed:
                           # Candidate failed to juxtapose? Heavier penalty.
                          structurally_matched += 0.2
            except ValueError: # One word missing in ref or cand
                 pass


    if expected_in_ref == 0:
         # If no known compounds expected, check if candidate *introduced* any Noun-Noun pairs unnecessarily
         # (Harder check, requires POS tagging) - return 1.0 for now.
         return 1.0

    # Score based on finding the components and matching the structure (juxtaposed or not)
    coverage_score = found_in_cand / expected_in_ref if expected_in_ref > 0 else 1.0
    structure_score = structurally_matched / found_in_cand if found_in_cand > 0 else 1.0 if expected_in_ref == 0 else 0.0

    # Combine coverage and structural match
    final_score = structure_score * (0.5 + 0.5 * coverage_score)
    return final_score